{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbd_dir='C:\\programowanie\\python\\deep learning\\\\6\\data\\\\aclImdb'\n",
    "train_dir=os.path.join(imbd_dir,'train')\n",
    "\n",
    "label=[]\n",
    "texts=[]\n",
    "\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name=os.path.join(train_dir,label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:]=='.txt':\n",
    "            f=open(os.path.join(dir_name,fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type=='neg':\n",
    "                label.append(0)\n",
    "            else:\n",
    "                label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=100\n",
    "training_samples=200\n",
    "validation_sample=10000\n",
    "max_words=10000\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences=tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "print(f'Znaleziono {len(word_index)} unikatowych tokenów')\n",
    "\n",
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "\n",
    "label=np.array(label)\n",
    "print(data.shape)\n",
    "print(label.shape)\n",
    "\n",
    "indices=np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data=data[indices]\n",
    "label=label[indices]\n",
    "\n",
    "xtrain=data[:training_samples]\n",
    "ytrain=label[:training_samples]\n",
    "xval=data[training_samples:training_samples+validation_sample]\n",
    "yval=label[training_samples:training_samples+validation_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir='C:\\programowanie\\python\\deep learning\\\\6\\data'\n",
    "#nlp.stanford.edu/projects/glove\n",
    "embedding_index=[]\n",
    "f=open(os.path.jpon(glove_dir,'glove.6B.100d.txt'))\n",
    "\n",
    "for line in f:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    coefs=np.asarray(values[1:],dtype='float32')\n",
    "    embedding_index[word]=coefs\n",
    "f.close()\n",
    "print(f'Znaleziono {len(embedding_index)} unikatowych tokenów')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim= 100\n",
    "embedding_matrix=np.zeros((max_words,embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if i< max_words:\n",
    "        if embedding_vector is not None:\n",
    "            embedding_index[i]=embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(max_words,embedding_dim,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable=False\n",
    "model.compile(optimizer='rmsprop',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['acc'])\n",
    "history=model.fit(xtrain,ytrain,\n",
    "                  epochs=10,\n",
    "                  batch_size=32,\n",
    "                  validation_data=(xval,yval)  )\n",
    "model.save_weights('pre_trainded_glove_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f9c62f4ec1edd975e1b5f8110d7cf5153d4e6ef454a08f3a37cf01ef6e18f5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
